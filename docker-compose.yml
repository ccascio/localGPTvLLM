services:
  # Note: vLLM service is not included here as it's typically run separately
  # with specific model configurations. Start your vLLM server manually with:
  # python -m vllm.entrypoints.openai.api_server --model your-model-name --host 0.0.0.0 --port 8000

  # RAG API server
  rag-api:
    build:
      context: .
      dockerfile: Dockerfile.rag-api
    container_name: rag-api
    ports:
      - "8001:8001"
    environment:
      # Use host vLLM server by default
      - VLLM_HOST=${VLLM_HOST:-http://host.docker.internal:8000}
      - VLLM_API_KEY=${VLLM_API_KEY}
      - OLLAMA_HOST=${VLLM_HOST:-http://host.docker.internal:8000}  # Backward compatibility
      - NODE_ENV=production
    volumes:
      - ./lancedb:/app/lancedb
      - ./index_store:/app/index_store
      - ./shared_uploads:/app/shared_uploads
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/models"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - rag-network

  # Backend API server
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: rag-backend
    ports:
      - "8000:8000"
    environment:
      - NODE_ENV=production
      - RAG_API_URL=http://rag-api:8001
      - VLLM_HOST=${VLLM_HOST:-http://172.18.0.1:8000}
      - VLLM_API_KEY=${VLLM_API_KEY}
      - OLLAMA_HOST=${VLLM_HOST:-http://172.18.0.1:8000}  # Backward compatibility
    volumes:
      - ./backend:/app/backend
      - ./shared_uploads:/app/shared_uploads
    depends_on:
      rag-api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - rag-network

  # Frontend Next.js application
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: rag-frontend
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - rag-network

# volumes:
  # Note: ollama_data volume removed since we're using vLLM instead

networks:
  rag-network:
    driver: bridge    